{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: pylab import has clobbered these variables: ['identity']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import scipy.optimize\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_mldata\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[0, 0],\n",
      "       [0, 1],\n",
      "       [1, 0],\n",
      "       [1, 1]]), array([[0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [1]]))\n",
      "(array([[0, 0],\n",
      "       [0, 1],\n",
      "       [1, 0],\n",
      "       [1, 1]]), array([[0],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1]]))\n",
      "(array([[0, 0],\n",
      "       [0, 1],\n",
      "       [1, 0],\n",
      "       [1, 1]]), array([[0],\n",
      "       [1],\n",
      "       [1],\n",
      "       [0]]))\n"
     ]
    }
   ],
   "source": [
    "def data2numpy(D):\n",
    "    X = np.array([r[0] for r in D])\n",
    "    y = np.array([r[1] for r in D])\n",
    "    return X, y\n",
    "# [([data points], [targets])]\n",
    "data_AND = data2numpy([\n",
    "    ([0, 0], [0]), \n",
    "    ([0, 1], [0]),\n",
    "    ([1, 0], [0]),\n",
    "    ([1, 1], [1]),\n",
    "])\n",
    "\n",
    "data_OR = data2numpy([\n",
    "    ([0, 0], [0]), \n",
    "    ([0, 1], [1]),\n",
    "    ([1, 0], [1]),\n",
    "    ([1, 1], [1]),\n",
    "])\n",
    "\n",
    "data_XOR = data2numpy([\n",
    "    ([0, 0], [0]), \n",
    "    ([0, 1], [1]),\n",
    "    ([1, 0], [1]),\n",
    "    ([1, 1], [0]),\n",
    "])\n",
    "\n",
    "print(data_AND)\n",
    "print(data_OR)\n",
    "print(data_XOR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9,)\n",
      "[[  4.96236164e-17]\n",
      " [  6.68424893e-16]\n",
      " [  6.40209269e-16]\n",
      " [  1.00000000e+00]]\n",
      "\n",
      "(9,)\n",
      "[[  2.77928243e-21]\n",
      " [  1.00000000e+00]\n",
      " [  1.00000000e+00]\n",
      " [  1.00000000e+00]]\n",
      "\n",
      "(9,)\n",
      "[[  2.68301822e-126]\n",
      " [  1.00000000e+000]\n",
      " [  1.00000000e+000]\n",
      " [  9.82616455e-015]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class sigmoid:\n",
    "    def f(z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    def df(z):\n",
    "        s = sigmoid.f(z)\n",
    "        return s * (1 - s)\n",
    "\n",
    "class identity:\n",
    "    def f(z):\n",
    "        return z\n",
    "    \n",
    "    def df(z):\n",
    "        return 1\n",
    "\n",
    "class FFNN():\n",
    "    def __init__(self, layers_dims=[], activation_functions=[], regularization=0.0001):\n",
    "        self.layers_dims = layers_dims\n",
    "        self.activation_functions = [None] + activation_functions\n",
    "        weights_size = 0\n",
    "        for i in range(1, len(self.layers_dims)):\n",
    "            weights_size += self.layers_dims[i-1] * self.layers_dims[i]\n",
    "        self.b_size = sum(layers_dims[1:])\n",
    "        self.weights = np.random.rand(self.b_size + weights_size) * 0.1\n",
    "        print(self.weights.shape)\n",
    "        self.training_iterations = 0\n",
    "        self.lambd = regularization\n",
    "    \n",
    "    def next_iteration(self, weights):\n",
    "        self.training_iterations += 1\n",
    "#         print(\"Iteration:\", self.training_iterations)\n",
    "    \n",
    "    def cost(self, weights, X, y):\n",
    "        nl = len(self.layers_dims) - 1\n",
    "        deltas = [None for i in range(nl + 1)]\n",
    "        \n",
    "        self.weights = weights\n",
    "        self.forward_pass(X, store_results=True)\n",
    "        a = self.a\n",
    "        z = self.z\n",
    "        B, W = self.unfold_weights(weights)\n",
    "        deltas[nl] = -(y - a[nl]) * self.activation_functions[nl].df(z[nl])\n",
    "        # deltas\n",
    "        for l in range(nl, 1, -1):\n",
    "            deltas[l-1] = deltas[l].dot(W[l]) * self.activation_functions[l-1].df(z[l-1])\n",
    "        weights_derivatives = []\n",
    "        b_derivatives = []\n",
    "        \n",
    "        # derivatives\n",
    "        for l in range(1, nl+1):\n",
    "            dW = deltas[l].T.dot(a[l-1])\n",
    "            weights_derivatives.append(dW.flatten())\n",
    "            b_derivatives.append(deltas[l].sum(axis=0).flatten())\n",
    "        cost = ((y - a[nl]) ** 2).mean()  # + self.lambd * (weights ** 2).sum()\n",
    "        gradient = np.hstack(b_derivatives + weights_derivatives)\n",
    "\n",
    "        assert gradient.shape == weights.shape\n",
    "        return cost, gradient\n",
    "    \n",
    "    def unfold_weights(self, weights):\n",
    "        used_count = self.b_size\n",
    "        used_bcount = 0\n",
    "        W = [None]\n",
    "        B = [None]\n",
    "        for i in range(1, len(self.layers_dims)):\n",
    "            m, n = self.layers_dims[i], self.layers_dims[i-1]\n",
    "            w = self.weights[used_count:used_count + (m * n)]\n",
    "            w = w.reshape((m, n))\n",
    "            b = self.weights[used_bcount:used_bcount + m]\n",
    "            W.append(w)\n",
    "            B.append(b)\n",
    "            used_count += m * n\n",
    "            used_bcount += m\n",
    "        return B, W\n",
    "            \n",
    "    def forward_pass(self, X, store_results=False):\n",
    "        used_count = self.b_size\n",
    "        used_bcount = 0\n",
    "        Xl = X\n",
    "        B, W = self.unfold_weights(self.weights)\n",
    "        if store_results:\n",
    "            self.a = [X]\n",
    "            self.z = [X]\n",
    "        for i in range(1, len(self.layers_dims)):\n",
    "            zl = Xl.dot(W[i].T) + B[i]\n",
    "            Xl = self.activation_functions[i].f(zl)\n",
    "            if store_results:\n",
    "                self.z.append(zl)\n",
    "                self.a.append(Xl)\n",
    "        return Xl \n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        res = scipy.optimize.minimize(\n",
    "            fun=self.cost,\n",
    "            x0=self.weights,\n",
    "            args=(X, y),\n",
    "            method='L-BFGS-B',\n",
    "            jac=True,\n",
    "            tol=1e-30,\n",
    "            options={'maxiter': 100, 'disp': True},\n",
    "            callback=self.next_iteration,\n",
    "        )\n",
    "        self.weights = res.x\n",
    "\n",
    "\n",
    "for X, y in [data_AND, data_OR, data_XOR]:    \n",
    "# for X, y in [data_XOR]:        \n",
    "\n",
    "    nn = FFNN([2, 2, 1], [sigmoid, sigmoid, sigmoid])\n",
    "#     nn = FFNN([2, 4, 1], [identity, identity, identity])\n",
    "#     print(nn.cost(nn.weights, X, y))\n",
    "    nn.fit(X, y)\n",
    "    print(nn.forward_pass(X))\n",
    "#     print(nn.cost(nn.weights, X, y))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Digits classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mnist = fetch_mldata('MNIST original', data_home='./data')\n",
    "y_all = mnist.target[:, np.newaxis]\n",
    "intercept = np.ones_like(y_all)\n",
    "data = np.hstack([intercept, mnist.data, y_all])\n",
    "np.random.shuffle(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize_features(train, test):\n",
    "    \"\"\"Normalizes train set features to a standard normal distribution\n",
    "    (zero mean and unit variance). The same procedure is then applied\n",
    "    to the test set features.\n",
    "    \"\"\"\n",
    "    train_mean = train.mean(axis=0)\n",
    "    # +0.1 to avoid division by zero in this specific case\n",
    "    train_std = train.std(axis=0) + 0.1\n",
    "    \n",
    "    train = (train - train_mean) / train_std\n",
    "    test = (test - train_mean) / train_std\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data_count = 60000\n",
    "Xt = data[:train_data_count, :-1]\n",
    "yt = data[:train_data_count, -1:].astype(int)\n",
    "\n",
    "Xv = data[train_data_count:, :-1]\n",
    "yv = data[train_data_count:, -1:].astype(int)\n",
    "\n",
    "Xt, Xv = normalize_features(Xt, Xv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax FNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(203786,)\n",
      "Training accuracy: 0.1134\n",
      "Test accuracy: 0.1073\n"
     ]
    }
   ],
   "source": [
    "class FFNN():\n",
    "    def __init__(self, layers_dims=[], activation_functions=[], regularization=1.1):\n",
    "        self.layers_dims = layers_dims\n",
    "        self.activation_functions = [None] + activation_functions\n",
    "        weights_size = 0\n",
    "        for i in range(1, len(self.layers_dims)):\n",
    "            weights_size += self.layers_dims[i-1] * self.layers_dims[i]\n",
    "        self.b_size = sum(layers_dims[1:])\n",
    "        self.weights = np.random.rand(self.b_size + weights_size) * 0.0001\n",
    "        print(self.weights.shape)\n",
    "        self.training_iterations = 0\n",
    "        self.lambd = regularization\n",
    "    \n",
    "    def next_iteration(self, weights):\n",
    "        self.training_iterations += 1\n",
    "        if self.training_iterations % 10 == 0:\n",
    "            print(\"Iteration:\", self.training_iterations, flush=True)\n",
    "            yvp = nn.forward_pass(Xv)\n",
    "            yvp = yvp.argmax(axis=1)\n",
    "            print('Training accuracy: {}'.format(accuracy_score(yt, ytp)), flush=True)\n",
    "    \n",
    "    def cost(self, weights, X, y):\n",
    "        nl = len(self.layers_dims) - 1\n",
    "        deltas = [None for i in range(nl + 1)]\n",
    "        \n",
    "        self.weights = weights\n",
    "        self.forward_pass(X, store_results=True)\n",
    "        a = self.a\n",
    "        z = self.z\n",
    "        B, W = self.unfold_weights(weights)\n",
    "        deltas[nl] = -(y - a[nl])\n",
    "        deltas[nl] *= self.activation_functions[nl].df(z[nl])\n",
    "        # deltas\n",
    "        for l in range(nl, 1, -1):\n",
    "            deltas[l-1] = deltas[l].dot(W[l]) * self.activation_functions[l-1].df(z[l-1])\n",
    "        weights_derivatives = []\n",
    "        b_derivatives = []\n",
    "        \n",
    "        # derivatives\n",
    "        for l in range(1, nl+1):\n",
    "            dW = deltas[l].T.dot(a[l-1])\n",
    "            weights_derivatives.append(dW.flatten())\n",
    "            b_derivatives.append(deltas[l].sum(axis=0).flatten())\n",
    "        cost = ((y - a[nl]) ** 2).sum() + self.lambd * (weights ** 2).sum()\n",
    "        gradient = np.hstack(b_derivatives + weights_derivatives)\n",
    "\n",
    "        assert gradient.shape == weights.shape\n",
    "        return cost, gradient\n",
    "    \n",
    "    def unfold_weights(self, weights):\n",
    "        used_count = self.b_size\n",
    "        used_bcount = 0\n",
    "        W = [None]\n",
    "        B = [None]\n",
    "        for i in range(1, len(self.layers_dims)):\n",
    "            m, n = self.layers_dims[i], self.layers_dims[i-1]\n",
    "            w = self.weights[used_count:used_count + (m * n)]\n",
    "            w = w.reshape((m, n))\n",
    "            b = self.weights[used_bcount:used_bcount + m]\n",
    "            W.append(w)\n",
    "            B.append(b)\n",
    "            used_count += m * n\n",
    "            used_bcount += m\n",
    "        return B, W\n",
    "            \n",
    "    def forward_pass(self, X, store_results=False):\n",
    "        used_count = self.b_size\n",
    "        used_bcount = 0\n",
    "        Xl = X\n",
    "        B, W = self.unfold_weights(self.weights)\n",
    "        if store_results:\n",
    "            self.a = [X]\n",
    "            self.z = [X]\n",
    "        for i in range(1, len(self.layers_dims)):\n",
    "            zl = Xl.dot(W[i].T) + B[i]\n",
    "            Xl = self.activation_functions[i].f(zl)\n",
    "            if store_results:\n",
    "                self.z.append(zl)\n",
    "                self.a.append(Xl)\n",
    "        return Xl \n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        res = scipy.optimize.minimize(\n",
    "            fun=self.cost,\n",
    "            x0=self.weights,\n",
    "            args=(X, y),\n",
    "            method='L-BFGS-B',\n",
    "            jac=True,\n",
    "            tol=1e-5,\n",
    "            options={'maxiter': 500, 'disp': True},\n",
    "            callback=self.next_iteration,\n",
    "        )\n",
    "        self.weights = res.x\n",
    "\n",
    "\n",
    "\n",
    "le = preprocessing.LabelBinarizer()\n",
    "nn = FFNN([785, 256, 10], [sigmoid, sigmoid, sigmoid])\n",
    "# train \n",
    "ytt = le.fit_transform(yt)\n",
    "nn.fit(Xt, ytt)\n",
    "ytp = nn.forward_pass(Xt)\n",
    "ytp = ytp.argmax(axis=1)\n",
    "# test\n",
    "yvt = le.transform(yv)\n",
    "yvp = nn.forward_pass(Xv)\n",
    "yvp = yvp.argmax(axis=1)\n",
    "\n",
    "print('Training accuracy: {}'.format(accuracy_score(yt, ytp)))\n",
    "print('Test accuracy: {}'.format(accuracy_score(yv, yvp)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.70855\n",
      "Test accuracy: 0.6982\n"
     ]
    }
   ],
   "source": [
    "ytp = nn.forward_pass(Xt)\n",
    "ytp = ytp.argmax(axis=1)\n",
    "# test\n",
    "yvt = le.transform(yv)\n",
    "yvp = nn.forward_pass(Xv)\n",
    "yvp = yvp.argmax(axis=1)\n",
    "\n",
    "print('Training accuracy: {}'.format(accuracy_score(yt, ytp)))\n",
    "print('Test accuracy: {}'.format(accuracy_score(yv, yvp)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
